---
title: "Easy Larger-than-RAM data manipulation with disk.frame - useR! 2020 tutorial proposal"
author: "ZJ"
date: "11/25/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Title

Handling 100GBs of data with ease - Larger-than-RAM data manipulation with disk.frame

## Audience

The tutorial aims to introduce {disk.frame} to users with the needs to manipulate large amounts of data. The audience will benefit the most from this tutorial if they are already familiar with popular R data frameworks such as dplyr and {data.table} because {disk.frame} uses dplyr and data.table primitives like `select`, and `dt[i, j, by]`.

Some users rely on DBMS (e.g. PostgresSQL), Spark, or SAS to manage their large dataset. They will find tremendous benefit in switching to {disk.frame}, which will allow them to keep their workflow in R for as long as possible.

## Instructor background

I have more than ten years of experience using R and enterprise data manipulation tools like SAS and SQL. I have experience teaching Data Science courses in R in a group setting. I am the author of the {disk.frame} package, hence I have intimate knowledge of the inner workings of {disk.frame} and is, therefore, well-positioned to teach {disk.frame} for larger-than-RAM data manipulation.


The website I created for {disk.frame} is 
* https://diskframe.com and 

I have given a useR! 2019 talk on {disk.frame}, see
* https://www.youtube.com/watch?v=3XMTyi_H4q4&t=1s

My LinkedIn
* https://www.linkedin.com/in/daizj/

Here is an R course I wrote to Data Science in general
* https://github.com/xiaodaigh/intro_r_data_science

## Domain

Big Data manipulation - efficiently wrangle data that doesn't fit into RAM


## Points of appeal (75 words)
Every R user would have run into the "cannot allocate vector of size xxxB." error at some point. For most applications, R needs to load the data in its entirety into RAM. However, RAM is a precious resource and often do run out. 

{disk.frame} solves this issue by providing a framework to manipulate data on disk and minimize RAM usage. By using {disk.frame}, the user can perform analysis on much larger data than is normally possible with R. A modern laptop with {disk.frame} can comfortably handle 100GB's of data.


## Learning objectives: 100 â€“ 200 words
In this tutorial, the user will learn how to use {disk.frame} effectively to manipulate datasets up to 100GBs in size on a single computer. 

After the tutorial, the user should be able to 

* learn what the pros and cons of common "big data" technologies like DBMS, Spark, and {disk.frame} are
* understand how {disk.frame} can help with manipulating large amounts of data
* understand when to use {disk.frame} and when not to
* confidently use {disk.frame} for any data manipulation task and not worry about running out of RAM
* load large datasets into {disk.frame} format
* manipulate large {disk.frame} files
* summarize large {disk.frame} files

## Computing requirement
The requirement for {disk.frame} is R 3.4 and the ability to install packages via `install.packages`. A laptop is advised but not necessary.

If attendees wish to follow along, I recommend that they should have RStudio or Jupyter notebook set up. However, that is not necessary as I will running through code that they can obtain from Github.

## Teaching Assistant
I don't think Teaching Assistants are necessary. Installing disk.frame is simply running `install.packages("disk.frame")`.

However, anyone familiar with RStudio setup and installation of packages is welcome as Teaching Assistant. Any expertise in the installation and setting of Jupyter notebook or Jupyter Lab would be highly appreciated.

## Lesson Plan
The structure will be very informal, and questions are encouraged at all points along the way. 

* Introduction to disk.frame; how it's structured and why it's fast
* Pros and cons of disk.frame; when to use disk.frame and when not to
* Follow along: 
* generate a fake dataset that is reasonably large (so avoiding a large download from internet)
* Convert the dataset to CSV
* Show how to load the data from CSV to {disk.frame}
* Manipulate the {disk.frame}
* Do group-by and summary statistics with {disk.frame}
* show how dplyr and data.table syntax can operate on {disk.frame}
* Explain how to perform group-by efficiently 
* Demonstrate how joining works for large {disk.frame}s

**** break ****

After the break, we will cover advanced {disk.frame} topics

* Explain how to extend {disk.frame} with a custom function
* Explain how to tune {disk.frame} for maximum efficiency
* Show some convenience features for {disk.frame} in RStudio if time permits
* Conclusion and Q&A

## Other Considerations
The user can consult https://diskframe.com. It is regularly updated with relevant {disk.frame} resources like articles and how-to-guides.

The intention is to show how to manipulate larger-than-RAM data with {disk.frame}. However, we will not download any such data from the internet. Instead, we will use an existing function to generate such data. Hence, it will not pose any constraint on internet access nor speed. However, students wishing to follow along on laptops will need to install {disk.frame} via `install.packages("disk.frame")`.



