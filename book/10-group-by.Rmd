---
title: "Group-by"
author: "ZJ"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Group-by}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Group-by in `{disk.frame}`

The group-by framework of [`{disk.frame}`](https://diskframe.com) has been overhauled in v0.2.2. It is now able to perform some group-by-summarize operations in one stage. In this chapter we will cover 

1. How to use one-stage group-by
2. Manual Two-stage group and hard group-by
3. The architecture of `{disk.frame}` and its implications for group-by
4. How to define custom one-stage group-by functions and its limitatons


## One-stage Group-by

A one-stage group-by is the same as group-by for data.frames. This would be remarkable, if not for the limitaions imposed by the disk-based nature of `{disk.frame}`.  Before v0.2.2 of `{disk.frame}`, one-stage group-by was not possible, and the users had to rely to two-stage group-by even for simple operations like `mean`. 

However, now that one-stage group-by is possible, there are still limiations and not all functions are supported out-of-the-box. Hence, at the end of the chapter we have described [how to define custom one-stage group-by functions](## Custom one-stage group-by).

An example of one-stage group-by:

```r
result_from_disk.frame = iris %>% 
  as.disk.frame %>% 
  group_by(Species) %>% 
  summarize(
    mean(Petal.Length), 
    sumx = sum(Petal.Length/Sepal.Width), 
    sd(Sepal.Width/ Petal.Length), 
    var(Sepal.Width/ Sepal.Width), 
    l = length(Sepal.Width/ Sepal.Width + 2),
    max(Sepal.Width), 
    min(Sepal.Width), 
    median(Sepal.Width)
    ) %>% 
  collect
```

It is important to note that not all functions that can run in `data.frame` `summarize` would work automatically. This is because of how `{disk.frame}` works. Please see the secion on [defining your own one-stage-group-by](### Defining your own one-stage group-by) if you wish to learn how to define your own one-stage group-by functions.

### List of supported group-by functions

If a function you need/like is missing, please make a feature request [here](https://github.com/xiaodaigh/disk.frame/issues). It is a limitation that function that depend on the order a column can only obtained using estimated methods.

| Function | Exact/Estimate | Notes |
| -- | -- | -- |
| `min` | Exact |  |
| `max` | Exact |  |
| `mean` | Exact |  |
| `sum` | Exact |  |
| `length` | Exact |  |
| `n` | Exact |  |
| `n_distinct` | Exact |  |
| `sd` | Exact |  |
| `var` | Exact | `var(x)` only `cor, cov` support *planned*  |
| `any` | Exact |  |
| `all` | Exact |  |
| `median` | Estimate |  |
| `quantile` | Estimate | One quantile only |
| `IQR` | Estimate |  |

### Notes on One-Stage group-by

The results should be exactly the same as if applying the same group-by operations on a `data.frame`. If not then please [report a bug](https://github.com/xiaodaigh/disk.frame/issues).


## Group-by notes

The `disk.frame` implements the `chunk_group_by` operation with a significant caveat. In the `disk.frame` framework, group-by happens WITHIN each chunk and not ACROSS chunks. To achieve group by across chunk we need to put **all rows with the same group keys into the same file chunk**; this can be achieved with `hard_group_by`. However, the `hard_group_by` operation can be **VERY TIME CONSUMING** computationally and should be **avoided** if possible.

The `hard_group_by` operation is best illustrated with an example, suppose a `disk.frame` has three chunks
```
# chunk1 = 1.fst
#  id n
#1  a 1
#2  a 2
#3  b 3
#4  d 4

# chunk2 = 2.fst
#  id n
#1  a 4
#2  a 5
#3  b 6
#4  d 7

# chunk3 = 3.fst
#  id n
#1  a 4
#2  b 5
#3  c 6
```
and notice that the `id` column contains 3 distinct values `"a"`,`"b"`, and `"c"`. To perform `hard_group_by(df, by = id)` MAY give you the following `disk.frame` where all the `id`s with the same values end up in the same chunks. 

```
# chunk1 = 1.fst
#  id n
#1  b 3
#2  b 6

# chunk2 = 2.fst
#  id n
#1  c 6
#2  d 4
#3  d 7

# chunk3 = 3.fst
#  id n
#1  a 1
#2  a 2
#3  a 4
#4  a 5
#5  a 4
```

Also, notice that there is no guaranteed order for the distribution of the `id`s to the chunks. The order is random, but each chunk is likely to have a similar number of rows, provided that `id` does not follow a skewed distribution i.e. where a few distinct values make up the majority of the rows.

Typically, `chunk_group_by` is performed WITHIN each chunk. This is not an issue if the chunks have already been sharded on the `by` variables beforehand; however, if this is not the case then one may need a second stage aggregation to obtain the correct result, see *Two-stage group by*.

By forcing the user to choose `chunk_group_by` (within each chunk) and `hard_group_by` (across all chunks), this ensures that the user is conscious of the choice they are making. In `sparklyr` the equivalent of a `hard_group_by` is performed, which we should avoid, where possible, as it is time-consuming and expensive. Hence, `disk.frame` has chosen to explain the theory and allow the user to make a conscious choice when performing `group_by`.

```r
suppressMessages(library(disk.frame))
flights.df %>%
  hard_group_by(carrier) %>% # notice that hard_group_by needs to be set
  chunk_summarize(count = n(), mean_dep_delay = mean(dep_delay, na.rm=T)) %>%  # mean follows normal R rules
  collect %>% 
  arrange(carrier)
```

## Two-Stage Group-by

Prior to `{disk.frame}` v0.2.2, there is no general support for one-stage group-by. Hence a two-stage style group-by is needed. The key is understand is the `chunk_group_by` which performs `group-by` within each chunk.

For most group-by tasks, the user can achieve the desired result WITHOUT using `hard = TRUE` by performing the group by in two stages. For example, suppose you aim to count the number of rows group by `carrier`, you can set `hard = F` to find the count within each chunk and then use a second group-by to summaries each chunk's results into the desired result. For example,

```r
flights.df %>%
  chunk_group_by(carrier) %>% # `chunk_group_by` aggregates within each chunk
  chunk_summarize(count = n()) %>%  # mean follows normal R rules
  collect %>%  # collect each individul chunks results and row-bind into a data.table
  group_by(carrier) %>% 
  summarize(count = sum(count)) %>% 
  arrange(carrier)
```

Because this two-stage approach avoids the expensive `hard group_by` operation, it is often significantly faster. However, it can be tedious to write; and this is a con of the `disk.frame` chunking mechanism.

*Note*: this two-stage approach is similar to a map-reduce operation.

```r
suppressPackageStartupMessages(library(disk.frame))
setup_disk.frame()
```

```r
flights.df = as.disk.frame(nycflights13::flights)

flights.df %>%
  srckeep(c("year","distance")) %>%  # keep only carrier and distance columns
  chunk_group_by(year) %>% 
  chunk_summarise(sum_dist = sum(distance)) %>% # this does a count per chunk
  collect
```

This is two-stage group-by in action
```r
# need a 2nd stage to finalise summing
flights.df %>%
  srckeep(c("year","distance")) %>%  # keep only carrier and distance columns
  chunk_group_by(year) %>% 
  chunk_summarise(sum_dist = sum(distance)) %>% # this does a count per chunk
  collect %>% 
  group_by(year) %>% 
  summarise(sum_dist = sum(sum_dist))
```

You can mix group-by with other dplyr verbs as below, here is an example of using `filter`. 

```r
# filter
pt = proc.time()
df_filtered <-
  flights.df %>% 
  filter(month == 1)
cat("filtering a < 0.1 took: ", data.table::timetaken(pt), "\n")
nrow(df_filtered)
```

## Hard group-by

Another way to perform a one-stage `group_by` is to perform a `hard_group_by` on a `disk.frame`. This will rechunk the `disk.frame` by the by-columns. This is **not** recommended for performance reasons, as it can be quite slow to rechunk the file chunks on disk.
```r
pt = proc.time()
res1 <- flights.df %>% 
  srckeep(c("month", "dep_delay")) %>% 
  filter(month <= 6) %>% 
  mutate(qtr = ifelse(month <= 3, "Q1", "Q2")) %>% 
  hard_group_by(qtr) %>% # hard group_by is MUCH SLOWER but avoid a 2nd stage aggregation
  chunk_summarise(avg_delay = mean(dep_delay, na.rm = TRUE)) %>% 
  collect
cat("group-by took: ", data.table::timetaken(pt), "\n")

collect(res1)
```


## Custom one-stage group-by

### At a glance

`{disk.frame}` allows the user to enable create custom one-stage group-by functions. To make a function `fn` one stage. One needs to define two functions

1. `fn.chunk_agg.disk.frame` which applies the itself to each chunk
2. `fn.collected_agg.disk.frame` which accpets a `list` of returns from `fn.chunk_agg.disk.frame` and finalize the computation.

For example, to make `mean` a one-stage group-by function, `{disk.frame}` has defined `fn.chunk_agg.disk.frame` and `fn.collected_agg.disk.frame`, which we will illustrate with examples below.

But first, we shall explain some theory behind `{disk.frame}` to help you better understand "why does `{disk.frame}` do it like that?".

### How does `{disk.frame}` work

One may ask, how come only a few functions are supported for one-stage group-by? And why are some functions like `median` only produce estimates instead of producing the exact figure? To answer these question, we need to have an understanding of how `{disk.frame}` works.

A `disk.frame` is organized as chunks stored on disk. Each chunk is a file stored in [fst format](https://www.fstpackage.org/). The [`{future}` package](https://cran.r-project.org/web/packages/future/index.html) is used to apply the same function to each chunk, each of these operations are carried out in a separate R session. These R sessions cannot communicate with each other during the execution of the operations.

Once the operation has been performed the results will be bought back to the session from which the operation was called. This is the only point of interprocess communication. The process of making group-by in one stage does require some additional work.

To summarize, the two phases of a `df %>% some_fn %>% collect` operation is 

1. The `some_fn` is applied to each chunk, and the result is assumed to be a data.frame
2. `collect` then row-binds (`rbind`/`bind_rows`/`rbindlist`) the results together to form a data.frame in the main session

### How group-by works

Except for passing the result back to the main session, communication between worker sessions are not allowed. This limits how group-by operations can be performed, hence why group-by can be done in two stages for many functions. However, R's meta-programming abilities allows us to rewrite code to that automatically perform the two-stage group-bys. For example, consider:

```r
df %>% 
  group_by(grp1) %>% 
  summarize(sum(x)) %>% 
  collect
```

we can use meta-programming to transform that to 
  
```r
df %>% 
  chunk_group_by(grp1) %>% 
  chunk_summarize(__tmp1__= sum(x)) %>% 
  collect() %>% 
  group_by(grp1) %>% 
  summarize(x = sum(__tmp1__))
```

Basically, we are "compiling" one-stage group-by code to two-stage group-by code, and then executing it.

For `mean`, it's trickier, as one needs to keep track on the numerator and the denominator separately in computing `mean(x) = sum(x)/length(x)`. 

Therefore, `{disk.frame}` compiles 

```r
df %>% 
  group_by(grp1) %>% 
  summarize(meanx = mean(x)) %>% 
  collect
```

to

```r
df %>% 
  chunk_group_by(grp1) %>% 
  chunk_summarize(__tmp1__ = list(mean.chunk_agg.disk.frame(x))) %>% 
  collect %>% 
  group_by(grp1) %>% 
  chunk_summarize(meanx = mean.chunk_agg.disk.frame(__tmp1__))
```

where `mean.chunk_agg.disk.frame` defines what needs to be done to each chunk, as you can see, the return value is a vector where the elements are named `sumx` and `lengthx`. Here is an example implementation of `mean.chunk_agg.disk.frame`

```r
mean.chunk_agg.disk.frame <- function(x, na.rm = FALSE, ...) {
  sumx = sum(x, na.rm = na.rm)
  lengthx = length(x) - ifelse(na.rm, sum(is.na(x)), 0)
  c(sumx = sumx, lengthx = lengthx)
}

```

because the return value is not a scalar, we need to write it in a `list` (line 3). 

The `mean.collected_agg.disk.frame` receives a list of outputs from `mean.chunk_agg.disk.frame`. Recall that `mean.chunk_agg.disk.frame` returns a vector for each chunk, so the input to `mean.collected_agg.disk.frame` is a *list of vectors*

```r
mean.collected_agg.disk.frame <- function(listx) {
  sum(sapply(listx, function(x) x["sumx"]))/sum(sapply(listx, function(x) x["lengthx"]))
}
```

### How to define your own one-stage group-by function 

Now that we have seen two examples, namely `sum` and `mean`, we are ready summarize how group-by functions are implemented. 

Given the below

```r
df %>% 
  group_by(grp1) %>% 
  summarize(namex = fn(x)) %>% 
  collect
```

`{disk.frame}` compiles it to

```r
df %>% 
  chunk_group_by(grp1) %>% 
  chunk_summarize(__tmp1__ = list(fn.chunk_agg.disk.frame(x))) %>% 
  collect %>% 
  group_by(grp1) %>% 
  chunk_summarize(namex = fn.chunk_agg.disk.frame(__tmp1__))
```

Based on the above information, to make `fn` a one-stage group-by function, the user has to

1. Define `fn.chunk_agg.disk.frame` which is a function to be applied at each chunk
2. Define `fn.collected_agg.disk.frame` which is a function to be applied to *a `list` containing the returns from `fn.chunk_agg.disk.frame` applied on each chunk*

**Example of implementing `sum`**:

1. Define `sum.chunk_agg.disk.frame`

```r
sum.chunk_agg.disk.frame <- function(x, na.rm = FALSE) {
  sum(x, na.rm=na.rm)
}
```

2. Define `sum.collected_agg.disk.frame`, which needs to accept a list of `sum(x, na.rm)`, but `sum(x, na.rm)` is just a numeric, so

```r
sum.collected_agg.disk.frame <- function(list_sum) {
  sum(unlist(list_sum))
}
```

**Example of implementing `n_distinct`**:

The `n_distinct` function counts the number of distint values from a vector `x`

1. Define `n_distinct.chunk_agg.disk.frame`, to return a list of unique values. Because the same value can appear in multiple chunks, so to ensure that we don't double count, we simply return all the unique values from each chunk which is then deduplicated in the next phase

```r
n_distinct.chunk_agg.disk.frame <- function(x, na.rm = FALSE) {
  if(na.rm) {
    setdiff(unique(x), NA)
  } else {
    unique(x)
  }
}
```

2. Define `n_distinct.collected_agg.disk.frame`, which deduplicates the unique values

```r
n_distinct.collected_agg.disk.frame <- function(list_of_chunkwise_uniques) {
  dplyr::n_distinct(unlist(list_of_chunkwise_uniques))
}
```

### Limitations

We have seen that `{disk.frame}` performs operations in two phases 

1. apply the same function to each chunk
2. row-bind the results

and there are no communication between the sessions that applies the functions at chunk level.

Hence, it is generally difficult to compute rank based summarizations like `median` exactly. Hence most rank based calculations are estimates only. This is also true of distributed data system like Spark whose median function is also estimates only.


## Advertisements

### Interested in learning `{disk.frame}` in a structured course?

Please register your interest at:

https://leanpub.com/c/taminglarger-than-ramwithdiskframe

### Open Collective 

If you like disk.frame and want to speed up its development or perhaps you have a feature request? Please consider sponsoring {disk.frame} on Open Collective. Your logo will show up here with a link to your website.

#### Backers

Thank you to all our backers! 🙏 [[Become a backer](https://opencollective.com/diskframe#backer)]

<a href="https://opencollective.com/diskframe#backers" target="_blank"><img src="https://opencollective.com/diskframe/backers.svg?width=890"></a>

[![Backers on Open Collective](https://opencollective.com/diskframe/backers/badge.svg)](#backers)

#### Sponsors

 [[Become a sponsor](https://opencollective.com/diskframe#sponsor)]

 [![Sponsors on Open Collective](https://opencollective.com/diskframe/sponsors/badge.svg)](#sponsors) 

### Contact me for consulting

**Do you need help with machine learning and data science in R, Python, or Julia?**
I am available for Machine Learning/Data Science/R/Python/Julia consulting! [Email me](mailto:dzj@analytixware.com)
